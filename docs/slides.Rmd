---
title: "Analysis of Genetic Data 1: Inferring Population Structure"
author: Peter Carbonetto
output:
 beamer_presentation:
   template: beamer.tex
   keep_tex: false
---

Download 1000 Genomes data
==========================

We download the 1000 Genomes data from the European Bioinformatics
Institute. Downloading may take 10--20 minutes.

+ Long URL: ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/hd_genotype_chip

+ Short URL: http://bit.ly/2G7ZWYu
  
+ Download this file into the `data` folder: 
  `ALL.chip.omni_broad_sanger_combined.
   20140818.snps.genotypes.vcf.gz`

On the RCC cluster, you can run these commands:

```{bash get-1kg-data, eval=FALSE}
cd data
wget http://bit.ly/2C617oO -O 1kg.vcf.gz
```

Explore the VCF file
====================

Let's run some simple shell commands to inspect the genotype data
stored in the VCF file.

```{bash inspect-1kg-vcf, eval=FALSE}
cd data
ls -lh 1kg.vcf.gz
zcat 1kg.vcf.gz | less -S # On Mac, use gzcat.
```

+ VCF reference: http://www.cog-genomics.org/plink2/formats#vcf

VCF files: concepts
===================

+ The Variant Call Format (VCF) is a text format for storing many
types of DNA variant data (e.g., SNPs, deletions, insertions), and
for annotating these variants.

+ It is one of the most commonly used data formats in genetics.

+ It is an inefficient way to store genotype data (this is why we
have compressed it).

+ See also:
    - https://vcftools.github.io 
    - https://samtools.github.io/hts-specs 
    - doi:10.1093/bioinformatics/btr330

Download PLINK
==============

We download the stable version of the PLINK software in the `bin
folder`.

+ URL: http://www.cog-genomics.org/plink2

On the RCC cluster, you can run these commands to download and test
the 64-bit Linux binary for PLINK 1.9b5.2:

```{bash get-plink, eval=FALSE}
cd bin
wget http://bit.ly/2nUxOQX -O plink.zip
unzip plink.zip
./plink --version
```

Convert VCF to PLINK
====================

Run this command to convert the genotypes from VCF to the PLINK text
format. This may take a few minutes to complete.

```{bash vcf2plink, eval=FALSE}
cd data
../bin/plink --vcf 1kg.vcf.gz --recode \
  --chr 1-22 --allow-extra-chr \
  --geno 0.01 --out 1kg
```

+ Command details:
    - Creates two new files: `1kg.map` and `1kg.ped`.
    - We retain only SNPs on (autosomal) chromosomes 1--22.
    - We remove any SNPs with >1% missing genotypes.
    - These steps are taken to simplify the analyses.

Explore PLINK files
===================

Let's run some simple shell commands to inspect the 1000 Genomes
genotype data stored in the PLINK files.

```{bash inspect-1kg-plink, eval=FALSE}
head 1kg.map
tail 1kg.map
wc -l 1kg.map
less -S 1kg.ped
wc -l -w 1kg.ped
```

Next, use these same commands to inspect the AffyMetrix Human Origins
data stored in `origins.map` and `origins.ped` (they are already
included in the git repository).

+ Columns in `.map` file: (1) chromosome; (2) marker id; (3)
genetic distance on chromosome, in cM; (4) base-pair position on
chromosome.
  
+ Columns in `.ped` file: (1) family id, (2) individual id, (3) father
id, (4) mother id, (5) gender, (6) phenotype measurement, (7---) SNP
genotypes.

PLINK files: concepts
=====================

+ The most commonly used format for storing human genoytpe data.

+ Less flexible than VCF.

+ The PLINK text format is easy to view and manipulate with simple
shell commands (e.g., `wc`, `grep`, `cat`, `cut`, `paste`).

+ For long-term storage, use PLINK binary (`.bed`) format. It is much
more efficient, but is not human readable.

+ See: http://www.cog-genomics.org/plink/1.9/formats#ped

Prepare the 1000 Genomes data
=============================

To speed up the data processing steps, we convert to binary PLINK
format, then we remove related samples (which standard population
structure analyses are not designed to handle).

1. Convert to PLINK binary format:
```
cd data
../bin/plink --file 1kg --make-bed --out 1kg
```

2. Inspect the 3 new files: `1kg.bed`, `1kg.bim`, `1kg.fam`.

3. Remove 29 of 31 related samples:
```
cut -f 1 20140625_related_individuals.txt \
  > temp.txt
paste temp.txt temp.txt > samples.txt
../bin/plink --bfile 1kg --make-bed \
   --remove samples.txt --out 1kg_unrelated
```

Merge with Affymetrix Human Origins data (part 1)
=================================================

In order to merge the data sets, we need to identify a set of SNPs
that is common to both data sets (see below for details), and
extract the genotypes for the common SNPs only.

1. Extract common SNPs from 1000 Genomes data:
```
cd data
../bin/plink --bfile 1kg_unrelated \
  --extract 1kg_origins_markers.txt \
  --make-bed --out 1kg_common
```

2. Extract common SNPs from Human Origins data, and remove 2 samples
that are included in both data sets:
```
../bin/plink --file origins \
  --extract 1kg_origins_markers.txt \
  --remove dupids.txt --make-bed \
  --out origins_common
```

Merge with Affymetrix Human Origins data (part 2)
=================================================

3. Merge the two data sets:
```
../bin/plink --bfile 1kg_common \
  --bmerge origins_common \
  --out 1kg_origins_combined
```

*Optional exercise:* It is good practice to double-check the output
after each processing step. Use the `cut` and `diff` commands to
compare the SNPs in `1kg_common` and `origins_common` and check that
the SNPs are in the same order.

Prune SNPs in LD
================

Many basic population structure analyses (e.g., PCA) assume that the
SNPs are independent. A common step is to "prune" SNPs that are
strongly correlated with each other (*i.e.*, in linkage
disequibilirium, or LD) to make analysis better supported.

```{bash ld-prune-plink, eval=FALSE}
cd data
../bin/plink --bfile 1kg_origins_combined \
  --indep-pairwise 1000 500 0.8
../bin/plink --bfile 1kg_origins_combined \
  --make-bed --extract plink.prune.in \
  --out 1kg_origins_pruned
```

Typically you will want to be more aggressive in pruning SNPs in LD.

Data preparation: take-home points
==================================

+ vcftools and PLINK have many commands for manipulating genotype data.

+ For more specialized edits, you can go for with basic shell
commands (e.g., `awk`, `cut`, `head`, `cat`, `paste`).

+ Often the majority of the effort goes toward data processing.
Careless data processing (or no data processing) can lead to a poor
quality analysis.

+ It is very common to introduce errors when merging multiple data
sets. Errors can be due to different genome assemblies, different
allele encodings, different genotyping error rates, *etc.*

+ The PLINK merge command will correct some of these errors, but not all
of them.

+ I was conservative in selecting SNP common to both data sets to
avoid introducing errors. As a result, we lost a lot of data after
merging.

+ *Important:* Record all your data processing steps.

Run PCA on combined data set
============================

Outline of the PCA analysis:

1. Convert genotype data to a (numeric) matrix.

2. Start up interactive R environment.

3. Load genotype matrix into R.

4. Fill in missing genotypes.

5. Compute PCs in R using the {\tt rsvd} package.

Convert genotype data to a matrix
=================================

The input to PCA must be an $n \times p$ matrix, where $n$ is the
number of samples and $p$ is the number of SNPs.

```{bash plink2matrix, eval=FALSE}
cd data
../bin/plink --bfile 1kg_origins_pruned \
  --recode A --out 1kg_origins_recoded
```

This command creates a new file `1kg_origins_recoded.raw`:

Start up interactive R environment
==================================

Move to the repository root, and start up R. On the RCC cluster, run
these commands:

```{bash load-R, eval=FALSE}
pwd # Should be ... genetic-data-analysis-1
module load R/3.4.3
export OPENBLAS_NUM_THREADS=4 # Optional.
R --no-save
```

The third command dramatically speeds up the PCA using OpenBLAS
multithreaded matrix operations.

Load genotype matrix into R (part 1)
====================================

Before continuing, check your working directory:

```{r getwd, eval=FALSE}
getwd() # Should be ... genetic-data-analysis-1
```

I wrote a function to rapidly load the genotype matrix using function
`fread` from the `data.table` package. If you are **not** on the RCC
cluster, you may need to install this package.

```{r load-pkgs, eval=FALSE}
install.packages("data.table")
library(data.table)
source("code/geno.utils.R")
```

Load the genotype matrix into R: 

```{r load-geno, eval=FALSE}
geno <-
  read.geno.raw("data/1kg_origins_recoded.raw")
```

Load genotype matrix into R (part 2)
====================================

Run a few commands to inspect the genoytpe data, e.g.:

```{r inspect-geno, eval=FALSE}
class(geno)
dim(geno)
geno[1:5,1:5]
```

Fill in missing genotypes
=========================

* We need to fill in the missing genotypes:
    100*mean(is.na(geno))
    p <- ncol(geno)
    x <- colMeans(geno,na.rm = TRUE)
    for (j in 1:p) {
      i         <- which(is.na(geno[,j]))
      geno[i,j] <- x[j]
    }
    sum(is.na(geno))
* Compute the first 10 PCs (components explaining the most variation
  in the genotypes):
    library(rsvd)
    out.pca <- rpca(geno,k = 10,center = TRUE,scale = FALSE,retx = TRUE)
    summary(out.pca)
    head(out.pca$x)
    pcs <- out.pca$x
    colnames(pcs) <- paste0("PC",1:10)
* Let's save the results of our analysis.
    save(file = "output/1kg_origins_pca.RData",
         list = c("out.pca","pcs"))

# Example R stuff

```{r cars, echo = TRUE}
summary(cars)
```

# Slide with Plot

```{r pressure}
plot(pressure)
```

# ggplot code

```{r createdata, echo=TRUE, eval=FALSE}
df <- data.frame(x = rnorm(1000))
x <- df$x
base <- ggplot(df, aes(x)) + geom_density()  + scale_x_continuous(limits = c(-5, 5))
base + stat_function(fun = dnorm, colour = "red")
```


# Another Plot

```{r plotit, echo=FALSE}
library(ggplot2)
df <- data.frame(x = rnorm(1000))
x <- df$x
base <- ggplot(df, aes(x)) + geom_density()  + scale_x_continuous(limits = c(-5, 5))
base + stat_function(fun = dnorm, colour = "red")
```